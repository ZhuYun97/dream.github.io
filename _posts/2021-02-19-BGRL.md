---
layout: post
title: The reading notes for Bootstrapped Representation Learning on Graphs(BGRL)
subtitle: unsupervised graph representation learning
cover-img: /assets/img/BGRL_model.png
thumbnail-img: /assets/img/BGRL_model.png
share-img: /assets/img/BGRL_model.png
tags: [deep learning, graph representation, unsupervised learning]
---
## Bootstrapped Representation Learning on Graphs

受到BYOL的启发，该论文BGRL也不需要使用negtive samples了，摆脱了耗显存这一瓶颈。BGRL在许多数据集上都取得了SOTA，并且我们使用了GAT作为encoder，发现它能进一步提升精度。在这些数据集上，我们都方法甚至可以超过有监督学习。

模型的结构与BYOL不太一样，他们取消了Projection Network，也就是说BGRL在进行predict之前，不使用Projection Network将表征映射到低维的空间中，进而简化predictor的工作。而BGRL之所以不是用Projection Network，是因为他们根据经验发现，在一般公开的图数据集，这是不需要的，而且在大多数情况下，由于提供了更间接的学习信号，实际上会减慢学习速度。

### Contributions

1.  提出了自监督图表示学习方法BGRL，该方法可以在多个具有挑战性的大型图数据集上取得了SOTA，证明了BYOL方法可以适用于图域上的大规模工作。
    
2.  BGRL不需要negative samples，从而解决了之前一些contrastive learning需要时间和空间是节点数量的二次方的问题
    
3.  通过实验发现了更长的训练时间和BN显著提高了自监督学习性能，此外他们是第一个使用自监督训练在obgn-arXiv数据集上进行测试的
    

### Model

与BYOL处理独立数据点的数据集不同，他们遵循之前的图表示学习方法并利用图中固有的拓扑结构。

![model](/assets/img/BGRL_model_ann.png)

一个Graph表示为$\mathbf{G}=(\mathbf{X}, \mathbf{A})$，节点特征矩阵$\mathbf{X} \in \mathbb{R}^{N \times F}$，邻接矩阵$\mathbf{A} \in \mathbb{R}^{N \times N}$

BGRL有两个graph encoder，online encoder $\mathcal{E}_{\theta}$和target encoder $\mathcal{E}_{\phi}$

流程
1. BGRL首先会使用两个graph augmentation functions $\mathcal{A}_{1}$ and $\mathcal{A}_{2}$对$\mathbf{G}$进行增强，分别得到$\mathbf{G}_{1}=\left(\tilde{\mathbf{X}}_{1}, \widetilde{\mathbf{A}}_{1}\right)$ and $\mathbf{G}_{2}=\left(\tilde{\mathbf{X}}_{2}, \widetilde{\mathbf{A}}_{2}\right)$
2. 使用online encoder和target encoder分别对输入进行encoding$\widetilde{\mathbf{H}}_{1}:=\mathcal{E}_{\theta}\left(\widetilde{\mathbf{X}}_{1}, \widetilde{\mathbf{A}}_{1}\right)$，$\widetilde{\mathbf{H}}_{2}:=\mathcal{E}_{\phi}\left(\tilde{\mathbf{X}}_{2}, \widetilde{\mathbf{A}}_{2}\right)$
3. 
> BGRL与BYOL不太一样，它没有使用Projection Network

将online representation输入到predictor $p_θ$对target representation进行预测, $\widetilde{\mathbf{Z}}_{1}:=p_{\theta}\left(\widetilde{\mathbf{H}}_{1}, \widetilde{\mathbf{A}}_{1}\right)$. the predictor 只在node level进行处理，不会对graph information进行处理(ie. operating over $\mathbf{H}_{1}$ only, and not $\mathbf{A}_{1}$).
4. 参数更新
使得predicted target representations $\widetilde{\mathbf{Z}}_{1}$中每个节点的feature尽可能接近true target representations $\widetilde{\mathbf{H}}_{2}$
更新online encoder的参数：
$$\ell(\theta, \phi)=-\frac{2}{N} \sum_{i=0}^{N-1} \frac{\widetilde{\mathbf{Z}}_{(1, i)} \widetilde{\mathbf{H}}_{(2, i)}^{\top}}{\left\|\widetilde{\mathbf{Z}}_{(1, i)}\right\|\left\|\widetilde{\mathbf{H}}_{(2, i)}\right\|}$$
$$\theta \leftarrow \operatorname{optimize}\left(\theta, \eta, \partial_{\theta} \ell(\theta, \phi)\right)$$
更新target encoder的参数（使用指数移动平均）：
$$
\phi \leftarrow \tau \phi+(1-\tau) \theta
$$
与BYOL类似，BGRL不回collapse到trivial solution

#### Graph augementation functions
他们希望产生两种语义相似的views，使用两种简单的增强方式，node feature masking和edge masking。这些操作是graph-wise的不是node-wise的。
1. Edge Masking
$
\text { sample a random masking matrix } \boldsymbol{R} \in\{0,1\}^{N \times \Lambda}
$其中每一项都采样自伯努利分布$
\widetilde{\boldsymbol{R}}_{i j} \sim \mathcal{B}\left(1-p_{r}\right) \text { if } \boldsymbol{A}_{i j}=1
$
$$
\widetilde{\boldsymbol{A}}=\boldsymbol{A} \circ \widetilde{\boldsymbol{R}}
$$
2. Node feature masking
first sample a random vector $\tilde{m} \in\{0,1\}^{F}$它的每一个维度都是从伯努利分布中独立提取出来的
$\tilde{m}_{i} \sim \mathcal{B}\left(1-p_{m}\right)$
$$
\tilde{\boldsymbol{X}}=\left[\boldsymbol{x}_{1} \circ \tilde{\boldsymbol{m}} ; \boldsymbol{x}_{2} \circ \tilde{\boldsymbol{m}} ; \cdots ; \boldsymbol{x}_{N} \circ \tilde{\boldsymbol{m}}\right]^{\top}
$$
> 他们只考虑简单的、标准的增广，以便分离和研究BGRL作为表征学习方法的效果，因为众所周知，更强的增广可以对所学表征的质量产生很大的影响，作者在3.1.3也使用了一些更好的增强方式


### Experiments
作者在7个transductive和inductive数据集上进行了实验，在预训练结束后使用linear model进行测试
#### Unsupervised training
使用AdamW optimizer with weight decay = 1e-5，使用Glorot initialization初始化模型参数，predictor使用单层的MLP。
target parameters φ is initialized to 0.99 and gradually increased to 1.0 over the course of training following a cosine schedule.
#### Evaluation of embedding
使用 Velickovic et al提出的the standard linear evaluation protocol对预训练模型进行评测，
我们首先以无监督的方式训练每个模型并计算每个节点的embedding。然后固定住encoder的参数，将得到的embedding输入到线性模型中，通过一个逻辑回归损失与l2正则化训练这个线性模型，没有任何梯度流回到graph encoder。

### Results
![results](/assets/img/BGRL_results.png)

### Conclusions
该方法是第一篇将BYOL应用到graph dataset的工作，对比之前的一些图上预训练无监督工作，该工作不需要negative samples，并且也不需要projection network，因此可以在更大规模大数据集上进行实验。此外，他们的方法可以自然地扩展到学习graph-level embedding（不见得吧）。

### Appendix
超参的设置
![appendix](/assets/img/BGRL_appendix.png)

